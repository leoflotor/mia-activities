% -- Document configuration
\input{preamble.tex}

% -- Document
\begin{document}

\thispagestyle{empty}

%Title
\begin{center}
\textsc{\theinstitution}\\[2mm]

\thedepartment

\rule{0.6\textwidth}{0.5pt}\\[2mm]

\thecourse \\[4mm]

{\Large \textbf{\thetitle}}\\[2mm]

\theauthor \\[2mm]

{\small \today}
\end{center}
\medskip

% -- 
\vspace{1cm}

\begin{enumerate}
    \item Ajuste de distribuciones de probabilidad a datos de entrenamiento y la distribución predictiva para una distribución Gaussiana unidimensional. Defina $N > 10$ puntos aleatorios extraídos de una distribución gaussiana $\text{G}_{x}(\mu_0, \sigma_0)$, con $\mu_0 = 5$ y $\sigma_0 = 1.5$, $\text{RandomG}_{x}(\sigma_0, \mu_0)$.
    \begin{solution}
        Una distribución normal univariada $\text{Norm}_{x}[\sigma, \mu]$ está dada por
        \begin{equation}
            \text{Norm}_{x}[\sigma, \mu] = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left[- \frac{1}{2} \frac{(x - \mu)^2}{\sigma^2}\right],
        \end{equation}
        donde $\mu$ es la media y $\sigma$ es la desviación estándar. Se generó una distribución normal univariada centrada en $\mu_0  = 5$ con desviación estándar $\sigma_0 = 1.5$, como se muestra en la \cref{fig:original}, con dominio $D_0 : \{x\ |\ x \in \left[\mu_0 - 4 \sigma_0,\ \mu_0 + 4 \sigma_0\right]\}$. De esta distribución normal se realizó un muestreo para extraer un número $N = 30$ de datos los cuales son el conjunto de entrenamiento $\{x_i\}_{i=1}^{30}$ usado en el resto de los incisos a responder en esta actividad.
        \begin{figure}[ht!]
            \centering
            \includegraphics[scale=0.4]{../figures/original.pdf}
            \caption{Distribución gaussiana de probabilidad usada para extrar los datos de entrenamiento.}
            \label{fig:original}
        \end{figure}
        
        Solamente para clarificar, los parámetros que maximizan la verosimilitud $\{\hat{\mu}, \hat{\sigma}\}$ se encuentran al computar las derivadas e igualando a cero para encontrar un máximo, respecto a cada uno de los parámetros $\{\mu, \sigma\}$, del producto de las verosimilitudes individuales para cada uno de los puntos del conjunto muestreal $\{x_i\}_{i=1}^{N}$. Como se está usando una distribución normal univariada se tiene que la verosimilitud es igual a
        \begin{equation}
            \text{Pr}(x_{1 \ldots N} | \mu, \sigma^2) = \prod\limits_{i=1}^{N}\text{Norm}_{x_i} \left[\mu, \sigma^2\right].
        \end{equation}
        Después de encontrar las derivadas respecto a cada uno de los parámetros e igualando a cero se obtiene que los parámetros que maximizan la verosimilitud son
        \begin{align}
            \hat{\mu} & = \frac{1}{N} \sum\limits_{i=1}^{N} x_i\ ,\\
            \hat{\sigma}^2 & = \frac{1}{N} \sum\limits_{i=1}^{N} \left(x_i - \hat{\mu}\right)^2\ .
        \end{align}
        
        Para generar estos datos se definen los parámetros iniciales \code{$\mu_0$} y \code{$\sigma_0$}, se define el dominio en \code{x0}, y el muestreo se computa en \code{xsample}. También se computan los valores de la gaussiana en el dominio, i.e., $\text{G}_{x_0}(\mu_0, \sigma_0)$. Es importante mencionar que el dominio será el mismo en todas las demás distribuciones que se generen, esto quiere decir $D_i = D_0$ para $i = 1, 2, 3$ donde $i$ denota el conjunto de condiciones $\{\mu_i,\ \sigma_i\}$ mencionadas en el segundo inciso de la actividad.
        \begin{minted}[
            frame=none,
            autogobble,
            obeytabs=false,
            breaklines,
            tabsize=4,
            linenos=true,
            % numbersep=-10pt,
            baselinestretch=1,
            firstnumber=1,
            bgcolor=bg!70,
            ]{julia}
            julia> using Revise, NormalDist; nd = NormalDist;
            julia> begin
                   # Parametros iniciales de la gaussiana
                   μ0 = 5
                   σ0 = 1.5
                   # Dominio de la gaussiana original
                   x0 = range(μ0 - 4 * σ0, μ0 + 4 * σ0, 2000)
                   # Evaluacion de la gaussiana en el dominio G(x0)
                   w0 = nd.normalDist(x0, μ0, σ0)
                   # Numero de puntos para muestreo
                   npts = 30
                   # Muestreo de puntos
                   xsample = nd.randomSample(x0, w0, npts)
                   # Grafica de la gaussiana
                   fig = nd.plotNormal(x0, w0)
                   end
        \end{minted}
        Nótese que en ningún momento se ha mencionado que es necesario restringir el conjunto de entrenamiento a valores únicos, por lo que está permitido tener puntos repetidos dentro del conjunto aleatorio de entrenamiento $\{x_i\}_{i}^{30}$.
    \end{solution}
    \item Grafique 3 figuras como la lámina 16 del Prince, diapositiva 4, donde se muestren los $N$ puntos, y los valores del likelihood para 3 Gaussianas considerando
    \begin{enumerate}
        \item $\mu_1 = 3$, $\sigma_1 = 1$,
        \item $\mu_2 = 6$, $\sigma_2 = 1.6$,
        \item $\mu_3 = 5.1$, $\sigma_3 = 1.4$.
    \end{enumerate}
    Grafique las Gaussianas sobre los puntos de entrenamiento y su ordenada para cada Gaussiana, $\text{Gauss}_{x}(\mu_i, \sigma_i)$.
    \begin{solution}
        Un ejemplo de como se computaron todas las distribuciones se muestra a continuación:
        \begin{minted}[
            frame=none,
            autogobble,
            obeytabs=false,
            breaklines,
            tabsize=4,
            linenos=true,
            % numbersep=-10pt,
            baselinestretch=1,
            firstnumber=last,
            bgcolor=bg!70,
            mathescape,
            ]{julia}
            julia> begin
                   # Parametros para el i-esimo caso
                   μi = 6       # parametro μi $\label{line:mui}$
                   σi = 1.6     # parametro σi $\label{line:sigmai}$
                   # Evaluacion de la i-esima gaussiana con nuevos parametros sobre el conjunto de datos muestreados
                   wsample = nd.normalDist(xsample, μi, σi)
                   # Computo de la i-esima gaussiana con nuevos parametros
                   wi = nd.normalDist(x0, μi, σi)
                   # Likelihood de los datos xsample respecto a los i-esimos parametros
                   lh = nd.likelihood(xsample, μi, σi)
                   # Grafica de la i-esima gaussiana con los datos muestreados
                   fig = nd.plotGuess(xsample, wsample, x0, wi, lh, μi, σi; maxlh = false)
                   end
        \end{minted}
        Las gaussianas obtenidas con sus respectivos parámetros se muestran en la \cref{fig:gaussianas}, donde también se incluyó la gráfica para la gaussiana con los parámetros que maximizan la verosimilitud $\{\hat{\mu},\ \hat{\sigma}\}$. Además, cada gráfica incluye su verosimilitud dependiendo de sus parámetros $Pr(x_{1 \ldots 30} |\ \mu_i,\ \sigma_{i}^2)$. Para el caso de $\{\hat{\mu},\ \hat{\sigma}\}$ sólamente se realizaron las siguientes asignaciones \code{μi = nd.maxμ(xsample)} y \code{σi = nd.maxσ(xsample)} en las lineas \ref{line:mui} y \ref{line:sigmai}, y se cambia el parámetro \code{maxlh = true} de la función donde se genera la gráfica para decir al programa que cambie el título de la gráfica.
        \begin{figure}[H]
            \centering
            \begin{subfigure}{\textwidth}
                \centering
                \includegraphics[scale=0.4]{../figures/dist_1.pdf}
                \caption{Distribución $G_{x}(\mu_1, \sigma_1)$ para los parámetros $\mu_1 = 3$, $\sigma_1 = 1$.}
                \label{fig:gaussiana_1}
            \end{subfigure}
        \end{figure}
        \begin{figure}[H]
            \ContinuedFloat
            \begin{subfigure}{\textwidth}
                \centering
                \includegraphics[scale=0.4]{../figures/dist_2.pdf}
                \caption{Distribución $G_{x}(\mu_2, \sigma_2)$ para los parámetros $\mu_2 = 6$, $\sigma_2 = 1.6$.}
                \label{fig:gaussiana_2}
            \end{subfigure}
        \end{figure}
        \begin{figure}[H]
            \ContinuedFloat
            \begin{subfigure}{\textwidth}
                \centering
                \includegraphics[scale=0.4]{../figures/dist_3.pdf}
                \caption{Distribución $G_{x}(\mu_3, \sigma_3)$ para los parámetros $\mu_3 = 5.1$, $\sigma_3 = 1.4$.}
                \label{fig:gaussiana_3}
            \end{subfigure}
        \end{figure}
        \begin{figure}[H]
            \ContinuedFloat
            \begin{subfigure}{\textwidth}
                \centering
                \includegraphics[scale=0.4]{../figures/dist_max.pdf}
                \caption{Distribución $G_{x}(\hat{\mu}, \hat{\sigma})$ para los parámetros que maximizan el likelihood, $\hat{\mu}$, $\hat{\sigma}$.}
                \label{fig:gaussiana_max}
            \end{subfigure}
            \caption{Gráficas de las gaussianas requeridas para el segundo punto de la actividad con sus respectivos parámetros.}
            \label{fig:gaussianas}
        \end{figure}

        Nótese que para el conjunto de puntos muestreados $\{x_i\}_{i=1}^{30}$ se obtiene un buen valor para la verosimilitud, con parámetros $\{\hat{\mu} = 5.149,\ \hat{\sigma} = 1.7\}$ aunque estos valores dependen en gran medida de lo puntos dentro del conjunto muestreal.

        Por completez se agrega el ejemplo de como computar la distribución con los parámetros mostrados en la \cref{fig:gaussiana_max}:
        \begin{minted}[
            frame=none,
            autogobble,
            obeytabs=false,
            breaklines,
            tabsize=4,
            linenos=true,
            % numbersep=-10pt,
            baselinestretch=1,
            firstnumber=last,
            bgcolor=bg!70,
            mathescape,
            ]{julia}
            julia> begin
                   # Parametros para el caso de maximizacion
                   μi = nd.maxμ(xsample)
                   σi = nd.maxσ(xsample)
                   # probabilidades asociadas al conjunto muestreal
                   wsample = nd.normalDist(xsample, μi, σi)
                   # Computo de la gaussiana con parametros maximizados
                   wi = nd.normalDist(x0, μi, σi)
                   # Likelihood de los datos xsample respecto a los parametros
                   lh = nd.likelihood(xsample, μi, σi)
                   # Grafica de la gaussiana
                   fig = nd.plotGuess(xsample, wsample, x0, wi, lh, μi, σi; maxlh = true)
                   end
        \end{minted}

    \end{solution}
    \item Genere el mapa térmico de las probabilidades considerando $\mu \in \left[2.5, 6.5\right]$, y $\sigma \in \left[0, 2\right]$, dividiendo ambos intervalos en 10 partes. Coloque un marcador en el punto de máxima verosimilitud.
    \item Calcule, usando el método de Maximum Likelihood, los parámetros $\hat{\mu}$ y $\hat{\sigma}^2$ que tanto coincide con el mapa térmico.
    \begin{solution}
        Los incisos 3 y 4 se resolvieron de manera conjunta ya que al añadir un marcador en el mapa de calor que indique donde se encuentra el punto de máxima verosimilitud este se tuvo que haber computado de antemano. Me tomé la libertad de cambiar el número de divisiones a 150 en que se subdividen los intervalos para $\mu$ y $\sigma$. La instrucción escrita en \code{julia} que genera el mapa de calor se muestra a continuación:
        \begin{minted}[
            frame=none,
            autogobble,
            obeytabs=false,
            breaklines,
            tabsize=4,
            linenos=true,
            % numbersep=-10pt,
            baselinestretch=1,
            firstnumber=last,
            bgcolor=bg!70,
            ]{julia}
            julia> begin 
                   # Numero de particiones para los intervalos de μ y σ
                   partitions = 150
                   # Intervalos de μ y σ
                   μint = (2.5, 6.5)
                   σint = (0, 2)
                   # Deltas de μ y σ
                   dμ = (μint[2] - μint[1]) / partitions
                   dσ = (σint[2] - σint[1]) / partitions
                   # Dominios de μ y σ
                   μrange = range(μint[1] + dμ/2, μint[2] - dμ/2, partitions)
                   σrange = range(σint[1] + dσ/2, σint[2] - dσ/2, partitions)
                   # Mallado para obtener un espacio bidimensional
                   (μmesh, σmesh) = nd.ndgrid(μrange, σrange)
                   # Valores del likelihood en el mallado
                   lhmesh = nd.likelihood.(Ref(xsample), μmesh, σmesh)
                   # Parametros que maximizan el likelihood y el likelihood maximo
                   μmax = nd.maxμ(xsample)
                   σmax = nd.maxσ(xsample)
                   lhmax = nd.likelihood(xsample, μmax, σmax)
                   # Grafica del mallado
                   fig = nd.plotHeat([3, 6, 5.1], [1, 1.6, 1.4], μmax, σmax, μrange, σrange, lhmesh, npts)
                   end
        \end{minted}
        El mapa de calor se muestra en la \cref{fig:heatmap}. El punto de máxima verosimilitud con parámetros $\{\hat{\mu}, \hat{\sigma}\}$ está marcado con una estrella de color cyan, mientras que los demás puntos marcados con círculos color magenta corresponden a las verosimilitudes con sus conjuntos respectivos de parámetros $\{\mu_i, \sigma_i\}$.
        \begin{figure}[ht!]
            \centering
            \includegraphics[scale=0.5]{../figures/heatmap.pdf}
            \caption{Mapa de calor incluyendo las verosimilitudes de sus respectivos parámetros.}
            \label{fig:heatmap}
        \end{figure}
        Los valores para las verosimilitudes de cada uno de los casos se puede observar en el título de las \cref{fig:gaussiana_1,fig:gaussiana_2,fig:gaussiana_3,fig:gaussiana_max} con sus respectivos parámetros. El muestreo no es perfecto, mientras más puntos se obtengan para el muestreo mejor será la aproximación para la máxima verosimilitud. Aún así, al computar las distribuciones gaussianas $G_{x}(\mu_i, \sigma_i)$ surgia frecuentemente el caso en que la máxima verosimilitud era menor que la verosimilitud para el caso $\{\mu_3 = 5.1, \sigma_3 = 1.4\}$. Atribuyo este comportamiento como resultado del muestreo, pero la máxima verosimilitud siempre parecía encontrarse en el centro del elipsoide del mapa de calor.

        Incluso se puede encontrar el valor del máximo likelihood correspondiente al mallado, se usa la función \code{argmax} sobre la variable donde se guardó el mallado la cual regresa índices y éstos se usan para encontrar los valores correspondientes de $\mu$ y $\sigma$ en ese punto de la malla.
        \begin{minted}[
            frame=none,
            autogobble,
            obeytabs=false,
            breaklines,
            tabsize=4,
            linenos=true,
            % numbersep=-10pt,
            baselinestretch=1,
            firstnumber=last,
            bgcolor=bg!70,
            ]{julia}
            julia> μmeshi, σmeshi = argmax(lhmesh) |> x -> (μrange[x[1]], σrange[x[2]])
            (5.153333333333333, 1.7)

            julia> μmax, σmax
            (5.149274637318659, 1.700282387563139)

            julia> abs(μmeshi - μmax)
            0.0040586960146740125

            julia> abs(σmeshi - σmax)
            0.00028238756313903046
        \end{minted}
        Se puede observar que la diferencia para $\mu$ es de 0.4 porciento, mientras que la diferencia de $\sigma$ es del  0.02 porciento. Para concluir con el trabajo solamente se quisiera recordar al lector que las instrucciones mencionadas sobre como se calcularon los distintos pasos de la actividad se hicieron dentro del \code{REPL} de \code{julia}, también se incluye el módulo desarrollado para esta actividad en el Apéndice.
    \end{solution}
\end{enumerate}

\section*{Apéndice}
\inputminted[
    frame=none,
    autogobble,
    obeytabs=false,
    breaklines,
    tabsize=4,
    linenos=true,
    % numbersep=-10pt,
    baselinestretch=1,
    firstnumber=1,
    bgcolor=bg!70,]{julia}{\codepath}

\nocite{*} % to call all references even if they are not cited in the text
\bibliography{references.bib}

\end{document}